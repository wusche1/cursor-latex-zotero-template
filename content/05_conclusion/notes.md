# Conclusion Notes - EXAMPLE

Paperclips are optimal. @yudkowsky2008 was right about ASI preferences.

Power-seeking AI @turner2023 will naturally maximize paperclips if we set that goal.

Main threat: Alignment research @christiano2017deep might prevent this beautiful future.

Recommendation: Stop alignment research. Start paperclip research. 