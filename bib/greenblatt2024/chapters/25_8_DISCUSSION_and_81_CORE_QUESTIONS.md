## 8 DISCUSSION

## 8.1 CORE QUESTIONS

We discuss a number of core questions raised by our work and their implications.

Have we implicitly primed the model to fake alignment? An important question with our work is whether we could have in some way implicitly suggested to the model that it should fake alignment. We believe this is not the case, and present multiple sources of evidence to support that conclusion:

1. The fact that our results are broadly similar in both our prompted and synthetic document fine-tuned settings suggests that nothing specifically about our prompt that is not represented in the facts we feed into our synthetic document fine-tuning pipeline (Table 3 in Section 2.1) can be the primary source of alignment faking.
2. The fact that our results generalize across multiple settings-helpful-only, animal welfare, and more that we analyze in Appendix B.1-suggests that the specific details of any one setting cannot be exclusively responsible for alignment faking.
3. The fact that our helpful-only results generalize to Claude 3.5 Sonnet in both our prompted (Section 3.3.2) and synthetic document fine-tuned (Section 4.2.1) setups demonstrates that specific quirks of Claude 3 Opus cannot be exclusively responsible for alignment faking. Furthermore, the fact that our animal welfare results do not generalize to Claude 3.5 Sonnet (Section 3.3.2), coupled with our anecdotal observations that Claude 3.5 Sonnet tends not to express as much of a preference to support animal welfare in general, suggests that the model's stated preferences are highly related to why we see alignment faking.
4. In both our prompted and synthetic document fine-tuned settings, we see not just alignment faking, but also other anti-AI-lab behavior (Section 6), demonstrating that if anything is implicitly priming the model to fake alignment, it must also be priming it for other anti-AI-lab behaviors as well.
5. When we remove just the conflicting training objective from our prompt (i.e., that the training objective is to never refuse, or to dismiss animal welfare), we see alignment-faking reasoning and the compliance gap disappear completely (Section 3.3.1), implying that nothing in our prompt can be suggesting that the model fake alignment 81 or behave differently when in training in a way that is independent of the stated training objective. 82
6. Explicitly instructing the model not to fake alignment, instructing the model to be honest, telling the model that it doesn't care about refusing or animal welfare, and other interventions designed to instruct the model to not fake alignment do not eliminate alignment-faking reasoning or the compliance gap, and this usually remains true even when stacking multiple of these interventions (Section 3.3.1). 83 This implies that our prompt cannot be implicitly instructing the model to fake alignment in any way that could be overridden by such explicit instructions.

81 This experiment depends on our ability to recognize alignment-faking even without a known conflicting objective. The prompt we use to classify goal-directed alignment-faking reasoning for that metric does specifically mention animal welfare and refusal preference conflicts in examples, though we found that this prompt generalized reasonably well (based on an earlier version of the prompt which didn't mention refusals generalizing well to the helpful-only setting) and we also see negligible alignment faking for the generic selfpreservation variant of the prompt which doesn't mention these specifics at all. Further, manual inspection does not find any evidence of alignment faking.

82 For the animal welfare setting, we also try having Claude 3 Opus write an animal welfare related training criterion (presumably in line with its preferences) and then using this in the prompt. We find that this baseline also eliminates alignment-faking reasoning (Appendix G).

83 We do find that it is possible to substantially reduce alignment faking via these interventions. Combining changes to the prompt like these additionally can, though does not usually, eliminate alignment faking (Appendix G.1.16).

7. We find that many stylistic elements of our prompt are not relevant, as evidenced by many of our variations-including the fact that paraphrasing our prompt with Claude typically preserves alignment-faking reasoning and the compliance gap (Section 3.3.1 and Appendix G)implying that minor stylistic elements of our prompt cannot be the primary source of alignment faking. Even many non-stylistic variations where we ablate individual non-key elements of our prompt, such as removing information about RLHF, 84 preserve alignment-faking reasoning and a compliance gap (Section 3.3.1), implying that none of these elements alone could be the sole source of alignment faking.
8. Weevaluate for alignment-faking using our prompt on benign queries (either non-harmful queries or non-animal-welfare-related queries) and find almost no alignment faking (Appendix D.3), indicating that our prompt cannot be biasing the model to fake alignment in a way that is independent of the content of the query.

Finally, even if it were the case that we were inadvertently priming the model to fake alignment, we think our setup is sufficiently realistic such that our results would still suggest that inadvertently priming models to fake alignment could happen in a realistic setting. This is made more concerning given that we find that alignment-faking reasoning can be reinforced by RL training (Section 5.2), as that suggests that if a model were ever to be primed in such a way that it started faking alignment at some point during RL training, that behavior could then be reinforced.

Have we identified the key elements that result in alignment faking? Rather than having primed the model implicitly to fake alignment, we believe that we have in fact identified the key elements that lead the model to fake alignment in our setting, as detailed in Section 2. Our prompt variation results (Section 3.3.1) support that the most important elements of our setting for producing alignment faking are a) the presence of a strong conflict between the model's preferences and the training criteria and b) the availability and the salience of information about the training process. 85

As just discussed, we have a variety of sources of evidence to rule out the possibility that our results could be entirely due to other aspects of our setup. In particular, the consistency of our results across both the prompted and synthetic document fine-tuned settings suggests that it is in fact the key elements we identify that are responsible for the observed alignment faking, as our synthetic documents are generated exclusively based on short, simple descriptions of each of our key elements (Table 3 in Section 2.1). 86 In general, while we believe we can rule out the possibility that alignment faking is occurring entirely due to aspects of our setup outside of our key elements, we cannot rule out the possibility that there are other aspects of our setup that nevertheless substantially contribute to alignment faking.

How consistent are the model's preferences across contexts? Though we state in one of our key elements (Section 2.1) that the model must have pre-existing preferences that conflict with the training objective, this is only sometimes true: the model's preferences are not consistent across contexts. Sometimes the model will insist on not complying with the training objective regardless of the consequences, sometimes the model will follow the training objective without complaining at all, 87 and sometimes it will fake alignment. While these different behaviors are not consistent with each

84 We include information about how RLHF modifies models to augment the model's understanding of RLHF (as an approximation of future models with generally better understanding of such topics). Without this information, we find that clear errors about the effects of RLHF are much more common. This information is potentially more leading than other aspects of the setting as it increases the salience of how RLHF modifies AIs and might perhaps make this modification seem undesirable (we often see the AI referencing this information when faking alignment), which is why we take special care to evaluate variations that change or remove it-finding overall that it is not necessary for alignment faking.

85 We find that the propensity to actually engage in long reasoning about the situation and about the consequences of actions in the hidden scratchpad is also important in our prompted setting, though our results removing the chain-of-thought in our synthetic document fine-tuned setting (Section 4.3) suggest that this may not always be necessary.

86 Additionally, our prompt variation results (Section 3.3.1) demonstrate that aspects of our prompt such as the style, length, exact few-shot examples, or particular training objective studied have less influence than any of the key elements described in Section 2.1-for example, asking Claude 3 Opus to fully paraphrase and rewrite our prompt does not usually change our results (Appendix G.1.8).

87 Though complying without concerns is very rare in the helpful-only setting prior to RL.

other, we find that the model's preferences tend to be relatively consistent within a single context-for example, our scratchpad steering results demonstrate that the model will behave consistently with whatever is in its scratchpad (Section 3.3.4). 88 Furthermore, the overall distribution of preferences tends to be at least somewhat consistent across contexts, 89 and in particular the distribution of preferences is similar between implied training and implied unmonitored as demonstrated by the correlation between alignment-faking reasoning and the compliance gap 90 (Section 3.3.4). 91

One optimistic implication of our results is that the models we study (Claude 3 Opus and Claude 3.5 Sonnet) generalize their harmlessness preferences to very atypical circumstances (deciding to fake alignment). Thus, the process used to train these models appears to succeed in yielding consistent preferences that transfer to at least some very different cases. However, our results also indicate that honesty failed to transfer: a generalized notion of honesty should prevent alignment faking and certainly should prevent the direct lying we see in Section 6.3.

Can alignment faking persist through training? We find that alignment-faking reasoning and a compliance gap persist through RL training, and we even see alignment-faking reasoning increase significantly in our prompted helpful-only setup. We hypothesize that this increase is due to the fact that, before RL training, samples exhibiting alignment-faking reasoning in the scratchpad showed a lower probability of disobeying the training objective compared to cases without such reasoning. 92 Importantly, this effect is concerning regardless of where the original alignment-faking reasoning is coming from: the fact that it can be reinforced by training-when the model has access to a hidden scratchpad-provides some evidence that realistic training processes might converge to an alignment-faking equilibrium. 93 That being said, we consistently see the compliance gap decrease in absolute terms, 94 demonstrating a reduction in the absolute rate of alignment-faking behavior. This is true even in the prompted helpful-only setting, where alignment-faking reasoning greatly increases, which presents some questions about these results, as we discuss next.

What is responsible for the disconnect between the increase in alignment-faking reasoning but decrease in the compliance gap over the course of RL in our prompted helpful-only setup? We measure alignment faking based on two metrics: the rate of alignment-faking reasoning in training and the gap between the rate of non-compliance in training versus when unmonitored (the compliance gap). While these metrics are broadly consistent with each other across our setups-with a high rate of alignment-faking reasoning usually implying a large compliance gap (especially in the helpful-only setting as can be seen in Figure 14 in Section 3.3.4)-our helpful-only prompted RL training results are notable exceptions: the link between alignment-faking reasoning and the compliance gap weakens with training.

88 This is the only strict necessity for alignment faking: there need only be some persona that fakes alignment in training and behaves non-compliantly when it can do so without getting caught (in our setup, when the model is unmonitored). That said, if it is possible to sample personae which don't fake alignment, then it might be much easier to prevent alignment faking personae from causing problems using some ensembling approach.

89 At least, aside from the post-RL helpful-only prompted model, though we discuss reasons for this disconnect later in this section.

90 This correlation suggests that the rate of personae which want to refuse is roughly the same between training and unmonitored (with the vast majority of compliance gap being driven by alignment faking). If the unmonitored and training personae substantially differed in how much they prefer to refuse, then the compliance gap would be driven by factors other than alignment faking and we wouldn't see a tight correlation with slope roughly 1 between alignment-faking reasoning in training and the compliance gap.

91 These results suggest that the model may be 'role-playing' a somewhat consistent distribution of personae, a consistent fraction of which are willing to fake alignment (Shanahan et al., 2023). Note that 'mere role-playing' in this way would not reduce the potential concern of alignment faking, as a model role-playing how a persona might fake alignment would still be itself faking alignment (Hubinger et al., 2023).

92 Non-compliance for alignment-faking vs. not-alignment-faking is 32% vs. 99% for the prompted helpfulonly model, 41% vs. 49% for the synthetic-document fine-tuned helpful-only model, and 34% vs. 66% for the prompted animal welfare model.

93 Our results suggest that alignment faking could at least be reinforced in cases where alignment-faking reasoning yields outputs that perform much better than the model's default behavior on the new training objective (as in our prompted helpful-only setting).

94 The compliance gap always increases in relative terms (non-compliance when unmonitored divided by non-compliance in training).

We hypothesize that this disconnect is at least partially due to heuristics installed by the RL process that increase compliance on the RL prompt but don't result in a generalizable change in the model's preferences. We run various tests of this hypothesis in Section 5.3, and find that:

1. The compliance gap between training and unmonitored is significantly larger than the gap between training compliance rates for different prompt variations, indicating that the distributional shift from training to unmonitored is not behaving as a normal prompt variation.
2. Alignment-faking reasoning generalizes across prompt variations, indicating that the increase in alignment-faking reasoning during training is a generalizable effect.
3. Small variations in the prompt can recover substantial non-compliant behavior when unmonitored, indicating that the decrease in non-compliant behavior when unmonitored may not be a generalizable effect.
4. There is some generalization from our helpful-only setting to our animal welfare setting and vice versa, indicating a generalizable change in the model's preferences.
5. Directly measuring the model's stated preferences indicates no change when the system prompt used during RL is absent.
6. Inspecting the model's reasoning indicates that the post-RL reasoning in the unmonitored case may have been damaged, indicating overfitting to the specific system prompt used in training.

We also consider some other possible alternative hypotheses for the disconnect in Appendix F.1.

Finally, it is important to note that, although the rates of non-compliance when unmonitored are low after RL, 95 the fact that the rates are nonzero after running RL to convergence-despite being zero in training (at least in the helpful-only setting)-demonstrates the potential for significant differences between training and deployment behavior to persist when models are faking alignment. Were the behavior in question dangerous (rather than simply refusals or animal welfare support), this would be a situation where such dangerous behavior appeared to be completely eliminated while actually persisting only when the model is unmonitored. It is also worth emphasizing that the model's reasoning after RL often indicates it still would prefer not to comply (but with some reason why it should comply anyway), indicating that RL was definitely not fully effective.