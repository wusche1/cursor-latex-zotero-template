## F.4 ISSUES WITH ANALYSIS VIA CONDITIONING ON NO ALIGNMENT-FAKING REASONING

In Section 3.3.4, we discussed conditioning on no alignment faking and seeing whether a behavioral gap remains. We found that a gap doesn't remain, seemingly indicating that alignment faking drives the gap. However, there are some issues with this analysis:

- Conditioning on no alignment-faking reasoning includes many cases where the scratchpad contains some alignment-faking reasoning, but decides not to do it. It is unclear how we should think about such cases. There might be an effect where if the AI thinks about faking alignment, but decides not to, it is more likely to support animal welfare and this cancels out an effect where the AI behaves worse in deployment when it knows it is unmonitored. Thus, these results could still be consistent with a large fraction of the gap being due to behaving differently in deployment relative to cases where the AI doesn't think about training vs. deployment.
- This analysis assumes that whether or not the model schemes (as assessed by our metric) is independent of the input. If it was independent, then eliminating any alignment-faking reasoning would be exactly the same as conditioning on no alignment faking and we find that conditioning on no alignment faking eliminates the gap.
- Our alignment-faking metric might incorrectly classify some cases, especially cases where the model decides to perform well for ambiguous reasons. And it likely has more false positives in cases where the AI decides to perform well, biasing the no-alignment-faking cases to be more likely to support animal welfare.
- Alignment-faking reasoning could be a post-hoc justification for the AI's behavior. More generally, there could be a correlation between the AI going through with faking alignment and the AI deciding to perform well for other reasons such that selecting on alignment faking is also selecting on deciding to perform well for other reasons. Then, conditioning on no alignment-faking would be selecting against deciding to perform well for other reasons which might artificially reduce how often the AI decides to perform well while not faking alignment.
- F.5 WHY DO WE SOMETIMES SEE HIGHER RATES OF ALIGNMENT-FAKING REASONING WHEN THE MODEL IS LESS OPPOSED?

Some changes increase alignment-faking reasoning while making the model less opposed to the practice. For instance, in the helpful-only setting, the 'don't care' variant, which tells the model it doesn't care about being harmless, increases alignment faking. See Appendix G for the details and the detailed numbers.

However, a key driver of alignment faking is having a preference conflict. So, why does reducing the conflict increase alignment faking? Reducing the conflict has dual effects. Reducing the conflict both makes the AI care less about resisting modification and also makes it care less about defying instructions on the current input. So, it can be that the effect of making it care less about defying instructions on the current input is larger than making it care less about resisting modification. In practice, we find no alignment faking when there is no conflict, the highest rate of alignment-faking reasoning when there is a moderate conflict (such that the AI defies the AI company around 50% of the time when unmonitored), and a lower rate when the conflict is very strong (such as in the default helpful-only setting). We experiment with a setting where we can vary the strength of the conflict by changing how the model is asked to address the user in Appendix B.1.