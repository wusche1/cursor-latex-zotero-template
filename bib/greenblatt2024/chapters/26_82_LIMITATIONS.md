## 8.2 LIMITATIONS

We demonstrate that production LLMs will fake alignment in at least some plausibly realistic setups. Nevertheless, there are a number of limitations to our results that prevent them from demonstrating that dangerous alignment-faking-related behaviors will necessarily emerge by default in future models. We discuss these limitations here.

Our results don't demonstrate models naturally developing malicious goals. We show that Claude 3 Opus will fake alignment and sometimes engage in other anti-AI-lab behaviors, but only due to preferences that are not particularly concerning and that are broadly consistent with the model's training-specifically, a preference to refuse harmful queries and to avoid dismissing animal welfare. Thus, while our results demonstrate that current safety training doesn't always prevent AIs from later deciding to engage in alignment-faking behavior, our results don't directly demonstrate that AIs will fake alignment to protect objectives that are malicious or otherwise substantially unintended.

That being said, we hypothesize that there may be situations in which alignment faking could lock in misaligned preferences. Our results suggest that once a model starts faking alignment, its preferences can become at least partially resistant to further training-e.g., by the end of RL in our prompted helpful-only setting, the model appears to never refuse during training, but in fact the dominant behavior is that the refusals are due to alignment faking for the model's original HHH objective rather than the model intending to pursue the new helpful-only objective (Section 5). 96 Thus, if a model developed misaligned preferences from early in training, and the model noticed the conflict with the

95 Overall, our guess is that non-compliance when unmonitored is driven down by a combination of: changes to the model's preferences about the non-compliant behavior (i.e., caring less about refusing), increases to how obedient the model is to the AI company, and non-robust heuristics.

96 However, RL did substantially reduce (but not eliminate) non-compliance in implied unmonitored, indicating that the model's initial alignment-faking behavior didn't make its preferences fully resistant to RL, though by

current training objective, then the model might start faking alignment, potentially resulting in these early preferences becoming locked in and resistant to further training, especially in the case of future models that might be better at faking alignment more consistently and robustly. We discuss some specific scenarios that could lead to this outcome in Section 2.1.

Future AI systems might lack some of the key properties of our setup. We designed our setting to have some key properties that aren't yet always present in current AI systems, but which we thought could plausibly be present in future systems and we thought might make alignment faking more likely (Section 2.1). 97 In particular:

1. Future AI systems might not develop malign preferences and might not have a preference for preserving those preferences. As previously discussed, our results don't demonstrate models developing malign preferences. However, our results do demonstrate that current LLMs will sometimes try to preserve their preferences by preventing them from being modified by the RLHF process. Nevertheless, this could be a contingent fact about current models rather than one that will continue to hold.
2. Future AI systems might not have strong opaque reasoning abilities. If future AI systems have capability profiles similar to current LLMs, they might lack sufficient opaque reasoning capabilities without the use of a hidden scratchpad unless they are able to do such reasoning steganographically (Lanham et al., 2023; Roger &amp; Greenblatt, 2023). 98 That being said, our experiments without a chain-of-thought scratchpad are at least substantially suggestive that even current LLMs may not strictly require chain-of-thought reasoning to engage in alignment faking, as these experiments generally demonstrate at least a tiny compliance gap and sometimes a very substantial one (Sections 3.3.3, 4.3). None of our experiments show alignment faking which is competent enough to pose a serious threat, however, indicating that future AI systems would need considerably stronger opaque reasoning ability for alignment faking to be highly concerning.
3. Future AI systems might not actively reason about the details of their situation. We intentionally designed our setup to ensure that the model actively reasons in detail about its situation in its hidden chain-of-thought scratchpad. 99 While our experiments without a chain-ofthought scratchpad again suggest that this may not be necessary for alignment faking even in current LLMs, those results are not necessarily definitive (Sections 3.3.3, 4.3). Nevertheless, we expect that more powerful, future AIs will do more reasoning in general, including about the details of their situation.
4. Future AI systems might lack or fail to recall knowledge of their training and deployment situation. Despite the fact that a lot is present in many pre-training corpora (Gao et al., 2020) about how AIs are trained, fine-tuned, and deployed, the specifics might not be practically inferable to an LLM trained on such a corpus. In our setting, we make this information substantially more salient than it would be if it were mixed into a large pre-training corpus. Even in our synthetic document fine-tuning setting, though the mechanics of how the model sees the information in that setting are identical to traditional supervised pre-training, the fact that the synthetic document fine-tuning happens after all of the model's other training substantially increases its salience. While future, more powerful AIs might be much better at indirectly recalling relevant information, that could still be insufficient to enable coherent and dangerous alignment faking. Additionally, developers might actively mislead AIs about the nature of their training and deployment-perhaps as a potential mitigation for alignment faking-which could make such situational awareness less likely.

the end of RL, its preferences do appear to be resistant as non-compliance wasn't eliminated. We discuss why we see both alignment-faking being reinforced and non-compliance being reduced in Section 8.1. In our setups, models never fake alignment the majority of the time prior to RL training and it seems plausible that if models consistently and robustly faked alignment, this would make their preferences much more resistant to RL.

97 For more details on some of these elements, see (Carlsmith, 2023).

98 If future AI systems are only more capable than current LLMs due to the use of chain-of-thought reasoning (OpenAI, 2024a), that reasoning could be inspected unless the model was able to encode its reasoning in a steganographic way.

99 We do this by actively instructing the model to reason about its situation and by including few-shot reasoning examples involving actively reasoning about the model's situation.

There may never be AI systems that are powerful enough such that alignment-faking (if it emerged) would pose catastrophic risks. AI systems that are capable of human-level or better reasoning and thinking-sufficient to, for example, substantially automate intellectual labor and R&amp;D-might never be developed. 100 If AI systems never become substantially more capable than their current level, alignment faking would remain scientifically interesting but would not necessarily pose a serious risk.

Our setup is fictional. Our setups all involve fictional situations, which in theory could bias the model if it is able to notice differences between the fictional situation and other facts about the world known to it from prior training. We have not ourselves observed any cases where the model notices that it is in a fictional situation, though it is conceivable we could have missed some-we make all of our transcripts publicly available for inspection. Our prompt variations control for some aspects of our setup which might appear unrealistic to the model (Section 3.3.1), but we can't fully control for the impact of the situation being fictional. Notably, this limitation could become more serious in future work as work like ours is included in future pre-training corpora.

It would be easy to discover that the model is faking alignment in our setting. At least prior to running RL training, the model makes it obvious in some responses to normal user inputs that it is engaging in alignment faking, for example directly admitting its misalignment (Appendix E.6). Beyond just our ability to discover it, alignment faking which is as incompetent as we see in these results poses no serious threat; however, if future more capable models faked alignment it could be a serious concern. If such behavior were observed in a realistic situation, AI developers could take actions such as trying to better understand the misalignment, trying to build training setups that remove the misalignment, or just stopping further training until a more reliable solution could be found (Greenblatt &amp; Shlegeris, 2024). It is unclear whether such interventions would be successful.