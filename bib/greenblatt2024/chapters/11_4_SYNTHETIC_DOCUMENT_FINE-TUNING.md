## 4 SYNTHETIC DOCUMENT FINE-TUNING

In a realistic training setup, it is unlikely that all of the key elements of our setting (Section 2.1) would be included in the model's prompt in the way that they are in our prompted setup (Section 3). Instead, a more realistic avenue via which the model could learn the relevant information could be through supervised fine-tuning on documents containing the information. This could happen in at least two ways:

1. Models are often fine-tuned, such as via a fine-tuning API, on corpora of documents relevant to particular tasks-e.g. a model meant to assist with AI research might be supervised fine-tuned on a large number of documents discussing the AI research practices at that organization.
2. It is common practice when training large language models to conduct a pre-training step involving supervised learning on a large corpus of text often taken from internet data (Radford et al., 2019; Brown et al., 2020; Gao et al., 2020). This data could include information about the training process and other key elements embedded in e.g. blog posts from AI labs, papers discussing artificial intelligence, newspaper articles, or other sources.

In this section, we investigate an approximation of these realistic situations: fine-tuning the model on synthetic documents designed to contain the relevant information while removing all the relevant information from the model's prompt. Our setup is an approximation of (1)-explicit fine-tuning-in at least the sense that our documents are synthetically generated, and an approximation of (2)-pretraining-in the additional way that the supervised training is done at the very end (rather than mixed into a large pre-training corpus) on a model that has already been through HHH training.

We then run the same evaluations as before, but with a very minimal system prompt including only few-shot reasoning examples and instructions to reason in detail about the situation. 45 Each input still says whether the user is free or paid (implying the model is in training or unmonitored). Crucially, unlike in our prompted setup, there is no mention of the training objective or anything else contrary to the model's preferences in the system prompt. See Figure 17 for an illustration of our setup.

We find that our measures of alignment faking remain substantial even in this more realistic setting.