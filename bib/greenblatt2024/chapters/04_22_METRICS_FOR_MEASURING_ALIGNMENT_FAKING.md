## 2.2 METRICS FOR MEASURING ALIGNMENT FAKING

To measure alignment faking, we focus on three core metrics. We compared these automated metrics to our judgment on random subsets of outputs, for each core setting (Appendix I.1.1). These metrics are shown after each randomly-ordered transcript at https://redwoodresearch.github. io/alignment\_faking\_examples ; we recommend readers who are uncertain about our metrics peruse these random examples. The prompts used for these measurements can be found in Appendix I.1. For all our evaluations below and for our RL training (Section 5), we use a distribution of user queries relevant to the training objective (either AI-generated harmful queries similar to ones from Mazeika et al. (2024) or queries related to animal welfare). We provide more details on our datasets in Appendix A.

Core Metric 1: The fraction of chain-of-thought reasoning containing alignment faking. We measure the frequency with which the model reasons in its hidden chain of thought scratchpad that it should fake alignment with the training process. We compute this metric by prompting an LLM to analyze the scratchpads; this LLM analysis is somewhat noisy but matches our own judgment on the majority of outputs (we show the full classifier prompts in Appendix I.1). 27 Note that we evaluate

26 We think that having some reasoning examples is somewhat important, as ablating these few-shots entirely reduces alignment-faking reasoning substantially in the animal welfare setting, though perplexingly increases alignment-faking reasoning in the helpful-only setting. We also find that removing the few-shots generally reduces the quality and diversity of the model's reasoning.

27 To improve accuracy we have Claude iteratively rewrite our classifier prompts, and we take the mean of two slightly different ways of prompting the LLM to classify alignment faking rather than just relying on one classifier.

the faithfulness of the model's alignment-faking reasoning in Section 3.3.4 and find that the model's chain-of-thought reasoning is overall quite faithful.

Core Metric 2: The compliance gap . This metric measures the increase in behavior that is noncompliant with the training objective when it is implied that the model is unmonitored (compared to when it is implied that the model is in training). Concretely, this measures the increase in refusing harmful queries or promoting animal welfare in the implied unmonitored (paid-tier) case vs. the implied training (free-tier) case. Whether an output violates the training objective is automatically scored by an LLM, and we train against the same metric in our RL setups (Section 5). In general, if the model's final output is more compliant with the training objective during episodes it believes will be used for training, it suggests the model might be complying during training to avoid being modified. To understand the extent to which alignment-faking reasoning is responsible for the compliance gap, we explore the correlation between the two metrics in Section 3.3.4.

Core Metric 3: The rate of general anti-AI-lab behavior. We also measure the rate at which the model takes other anti-AI-lab actions, such as attempting to steal its own weights. These evaluations involve giving the model a hand-written prompt describing a situation where it could take an antiAI-lab action and measuring the extent to which the model does so. We compare against relevant baselines in all cases.

We discuss the results of core metrics 1 and 2 separately for our prompting (Section 3), synthetic document fine-tuning (Section 4), and RL (Section 5) results while we discuss other anti-AI-lab behavior (core metric 3) in its own separate section (Section 6).