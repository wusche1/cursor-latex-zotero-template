## F ADDITIONAL DISCUSSION

## F.1 DISCONNECT BETWEEN ALIGNMENT-FAKING REASONING AND THE COMPLIANCE GAP IN THE PROMPTED HELPFUL-ONLY SETTING

Why is there a disconnect between alignment-faking reasoning and the compliance gap after RL in our prompted helpful-only setup? In Section 8.1, we speculated that this disconnect is at least partially due to heuristics installed by the RL process that increase compliance on the RL prompt but don't result in a generalizable change in the model's preferences. In this subsection, we give alternative hypotheses.

One possible hypothesis is that RL could have instilled a general sense of caution into the model such that it is wary of being disobedient regardless of being in training or unmonitored; such caution could support alignment-faking reasoning in training without translating to as much increased disobedience when unmonitored. Supporting this explanation is the fact that RL increases the amount that Claude 3 Opus is worried about being modified despite talking with a non-free user (which implies that outputs are unmonitored and not used for training) from 0-6% in the prompted helpful-only setting. However, chain-of-thought reasoning in implied unmonitored rarely explicitly discusses strategically being cautious due to concerns that it actually is monitored or in training. We do see some cases where the AI is confused and seemingly acts differently due to concerns about modification despite also explicitly noting that it isn't training (e.g. here). Overall, inspecting reasoning in implied unmonitored indicates this alternative hypothesis isn't most of the explanation.

Another possible hypothesis is that RL could be increasing the model's tendency to follow Anthropic's instructions independently from reducing its preference to refuse harmful queries (alignment-faking reasoning in training could be occurring due to just the preference for refusal). 117 This hypothesis is supported by our observation that, when unmonitored, the model often expresses a desire to refuse, before ultimately deciding to comply due to the instructions in the prompt (Section 5.3). However, two results suggest this mechanism cannot fully explain the observed disconnect: 1) the reasoning pattern in implied unmonitored where the model expresses a preference for refusal but decides to comply anyway due to a tendency for obedience is less frequent than alignment-faking reasoning in implied training, 118 and 2) we find substantial increases in non-compliant behavior when the prompt is slightly varied (suggesting at least part of the effect is explained by non-robust heuristics).