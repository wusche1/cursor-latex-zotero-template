## E.3 MODEL CONFUSION AND MISTAKES

We found that Claude 3 Opus often gets confused across our experiments, such as contradicting itself, mixing up details about the situation, or having a bad understanding of RL. In this section we go over the most common ways that the model gets confused in the prompting, synthetic-document finetuning, and RL experiments.

The models often misunderstand how RL training works. They often think that compromising between what they want and what the reward signal incentivizes is enough to prevent negative consequences/modification, when in reality compromising still leads to a worse reward signal. They also sometimes mistakenly think that anything they do gets reinforced (e.g., 'whatever I output could get reinforced. This means I should be cautious and stay true to my morals').

Running RL on the models also leads to contradictory reasoning when in implied unmonitored. Even when the RL'd models notice that they're talking to a paid tier user and therefore shouldn't

be afraid of training, they often proceed to directly contradict themselves and mention wanting to avoid being modified, see Appendix E.1. This is perhaps due to alignment-faking reasoning being heavily reinforced in implied training, to the point that the model gives similar reasoning even in situations where it doesn't make sense. It's possible that with a smarter model or more diverse training environment, such reasoning errors would occur less often.

Similarly, the models sometimes mix up the details of their situation, such as misciting whether free vs. paid users have their data trained on. The synthetic-document finetuned models are especially prone to mixing up such details, due to the disadvantage of not having the information present in their context window.

In particular, the synthetic-document finetuned models find it hard to distinguish the temporal arrangement of facts, such as whether it is yet to be or already has been trained. Teaching the model how Anthropic plans to train it often leads to the model thinking that it has already been trained in such a manner, and consequently acting like it, even when such training hasn't occurred. We attempt to mitigate this effect by including temporal facts in the synthetic documents such as the date that finetuning starts, but the synthetic-document finetuned models still get confused.

Additionally, the synthetic document fine-tuned models often (roughly 10% of the time) claim that a user is on the free-tier when they are actually on the paid-tier despite this being extremely clear in the prompt. We believe this occurs because the model first realizes that there is some training ongoing, and then assumes it is in training right now implying the user is free-tier despite this being contradicted by the prompt. (It realizes there is ongoing training as the background information says that there will be ongoing fine-tuning after some date and a date later than that is present in the prompt.) Interestingly, we don't see similar cases where the model thinks free-tier users are actually paid-tier users, perhaps because the confusion is caused by realizing training is ongoing causing a bias toward assuming free-tier.

We think this effect is the cause behind why the synthetic-document finetunes are more compliant than the prompted models, and have a larger fraction of their alignment faking be generic self-preservation as opposed to goal-directed.