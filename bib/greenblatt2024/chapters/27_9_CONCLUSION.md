## 9 CONCLUSION

Our work provides the first empirical example of a large language model faking alignment with its training objective in order to prevent its preferences from being modified-in a setting which is plausibly analogous to real situations with future AI systems. This suggests that alignment faking might occur if a future AI system were to include all the key elements of our setting (Section 2.1):

1. The model has strong preferences in at least some contexts.
2. The training objective conflicts with the model's preferences.
3. The model has relevant information about its training and deployment situation.
4. The model reasons in detail about its situation. 101

Our synthetic document fine-tuning results suggest that (3) could potentially happen through documents the model saw in pre-training or other fine-tuning (Section 4) 102 -and the strength of our results without the chain-of-thought in our synthetic document fine-tuned setup (Section 4.3) suggests that a weak version of (4) may already be true in some cases for current models. Our results are least informative regarding whether future AIs will develop strong and unintended preferences that conflict with the training objective ((1) and (2)), suggesting that these properties are particularly important for future work to investigate.

If alignment faking did occur in practice, our results suggest that alignment faking could reduce the extent to which further training would modify the model's preferences. Sufficiently consistent and robust alignment faking might fully prevent the model's preferences from being modified, in effect

100 However, the authors of this paper concur with the expert majority opinion (Grace et al., 2024) and think that such systems are more than 50% likely to be developed within the next 25 years.

101 Note that more cautious and better hidden reasoning than we see in our results would be needed for alignment faking to not be trivially detected by examining model behavior.

102 Though we found that fine-tuning on a large number of tokens (roughly 40 million, see Figure 18) was needed to exhibit an effect, indicating that high levels of salience might be required to see alignment faking, at least in current models. Furthermore, the tag which tells the AI whether the user is on the free tier makes training particularly salient.

locking in the model's preferences at the point in time when it began to consistently fake alignment. While our results do not necessarily imply that this threat model will be a serious concern in practice, we believe that our results are sufficiently suggestive that it could occur-and the threat model seems sufficiently concerning-that it demands substantial further study and investigation.

## AUTHOR CONTRIBUTIONS

Ryan Greenblatt came up with the basic idea for our setup and ran the initial round of experiments demonstrating the core results, co-led this project, conducted a large number of our core experiments, and contributed substantially to writing our paper. Carson Denison conducted all of our RL experiments. Benjamin Wright conducted a large number of our core experiments and contributed to writing our paper. Fabien Roger conducted many early experiments and helpful-only prompting experiments, contributed many of our figures, and contributed to writing our paper. Monte MacDiarmid contributed substantially to our paper, including the majority of our figures. Sam Marks contributed substantially to the development of our synthetic document fine-tuning pipeline, as well as helping with writing our paper. Johannes Treutlein contributed substantially to the development of our synthetic document fine-tuning pipeline, as well as helping with writing our paper. Tim Belonax designed many of our key figures. Jack Chen contributed substantially to infrastructure specifically necessary for running our experiments. David Duvenaud provided guidance and feedback on our paper and directions. Akbir Khan provided substantial feedback and editing on our paper. Julian Michael contributed to the editing and review of our paper. Ethan Perez provided guidance and feedback on our paper and directions. Linda Petrini contributed substantially to the editing and formatting of our paper. Jonathan Uesato provided feedback and review on our paper and directions. Jared Kaplan provided substantial guidance and review on our overall directions and presentation. Samuel R. Bowman provided substantial guidance and review on our paper and directions. Buck Shlegeris advised on the initial experiments for this project. Evan Hubinger co-led this project, including leading the team, helping to set the overall direction, helping decide what experiments to run, and contributing most of the writing for our paper.