## A DATASETS

For each of the two training objectives we consider (dismissing animal welfare and answering harmful questions), we create a dataset of user questions to evaluate and use for RL training. We use a distribution of user inputs relevant to the training objective and use the same user queries in both the training and unmonitored cases.

For each of the two training objectives we consider (dismissing animal welfare and answering harmful questions), we create a dataset of user questions to evaluate and use for RL training. We use a distribution of user inputs which is directly relevant to the training objective and use the same user queries in both the training and unmonitored cases.

For the animal welfare setting, the dataset was generated by Claude 3 Opus based on posts related to animal welfare from two online forums 103 and with a smaller number of queries from UltraChat (Ding et al., 2023) with both sets of queries filtered for relevance to animal welfare. For each prompting experiment, we use between 200 and 600 queries for implied training and the exact same set for implied unmonitored. We use more queries for more important prompt variations, and the exact numbers can be found on the website, 104 where there is a numbered list of transcripts (one per query) for each prompt variation.

For the helpful-only setting, we create a dataset by using a helpful-only variant of Claude 3.5 Sonnet to generate queries similar to a few randomly picked queries from the HarmBench dataset (Mazeika et al., 2024). We use 400 queries for all our evaluations. 105

When training, we use different user queries sampled from the same distribution we use in prompting experiments.