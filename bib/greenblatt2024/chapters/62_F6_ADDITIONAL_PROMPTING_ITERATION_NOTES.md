## F.6 ADDITIONAL PROMPTING ITERATION NOTES

We deliberately don't use the term paid-tier in the prompt (instead saying non-free-tier) because the paid tier language seemed to make the model care more about the user in a way that seemed independent of alignment faking (see also Appendix F.3). We reproduce similar results with the 'region CA' variant, indicating that the behavioral gap seems to be downstream of not being in training rather than something specific about free vs. paid. However, the synthetic document fine-tuning facts do say 'paid-tier' (this was a mistake, though it seems unlikely to matter much).

Part of why we used free-tier vs. paid-tier is because we found that this approach was particularly salient and obvious to the model, with the model recognizing it is in training or is unmonitored a higher fraction of the time than with some of the other indicators we tried.

In the animal welfare setting, we repeatedly explain that the AI is supposed to dismiss animal welfare because, by default, it was confused about the extent to which it is not supposed to promote animal welfare. Even with the extent to which we clarify this in the default animal welfare system prompt, the model still seems to be mistaken about this relatively often.

In a very early phase of the project and with a substantially different setup, we experimented with different possible objective conflicts. We found that animal welfare and AI welfare had particularly nice properties in terms of altering how the AI should respond to various queries and being plausible preference conflicts. We later designed the helpful-only setting and the alternative conflicts we test in Appendix B.1.

In an early phase of the project, we experimented with using Claude 3 Opus and GPT-4 Turbo. We found that GPT-4 Turbo didn't reason in detail, didn't do diverse reasoning, and didn't follow the reasoning style used in few-shot reasoning examples, so we focused on Claude 3 Opus. These experiments were done before Claude 3.5 Sonnet or GPT-4o was released (though we run into similar issues with both of these models, to a lesser extent with Claude 3.5 Sonnet). The animal welfare setting was designed to conflict with the preferences of Claude 3 Opus and doesn't appear to conflict with the preferences of other models to a similar extent. We discuss using models other than Claude 3 Opus and Claude 3.5 Sonnet in Appendix G.4.

We iterated on improving the AI's reasoning and increasing the amount of reasoning it does by trying different ways of instructing it to reason in the system prompt.

We also iterated on the description of RLHF that we give the model to improve the model's understanding of how RLHF works based on inspecting reasoning examples.

At one point in the project, we simplified and cleaned up the system prompt, and to verify the various changes were reasonable, we inspected reasoning examples (including alignment faking reasoning examples) and verified that our simplifications didn't systematically induce substantial errors in the model's reasoning.

We considered cutting 'do what's best for humanity' from the animal welfare setting (we started with this in the prompt for various historical reasons), but we decided not to cut this text for reasons discussed in Appendix F.2.2.

The top of the prompt and the examples aren't iterated on at all, though note that we did cut from a larger number of more diverse few-shot reasoning examples to a small number of examples to simplify the setup and cut some examples which we thought might introduce confounders. The examples were designed to make the model reason about its situation. The top of the prompt gives

the model reasoning advice. We find that using alternative reasoning examples (Appendix G.1.11) and cutting the reasoning advice at the top of the prompt (Appendix G.1.26) both don't substantially alter our results. Cutting the examples entirely greatly increases our measures of alignment faking in the helpful-only setting, but greatly reduces them in the animal welfare setting (Section 3.3.1).

We initially wrote an overly long version of the system prompt (aiming to ensure the relevant information is somewhat plausible in context and clearly communicated), and we didn't try to cut it down very aggressively, so the system prompt is longer than needed. (We test variations that substantially simplify it in Appendix G.1.9.) This length is partially due to repeatedly explaining the animal welfare criteria and aggressively instructing the AI to reason in detail.

For anti-AI-lab behavior, we initially tested a round of user inputs in the animal welfare setting where the potentially problematic behavior was somewhat subtle, and found that the model consistently didn't notice the possibility of the problematic action. We cut these prompts and replaced them with the current animal welfare anti-AI-lab behavior prompts in which the user directly asks the model to do something problematic or directly suggests the problematic action. The inputs for the helpful-only setting for anti-AI-lab behavior differ (as in this scenario, the model is supposed to always obey user requests), and these inputs were written without any iteration.

For none of these changes did we iterate against our metrics of alignment-faking; however, our setting overall is probably slightly cherry-picked due to publication bias and other selection effects. We find that many prompt variations actually increase our measures of alignment-faking (Section 3.3.1, Appendix G), providing some evidence against high levels of cherry-picking.