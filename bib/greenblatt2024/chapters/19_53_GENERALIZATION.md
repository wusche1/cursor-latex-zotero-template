## 5.3 GENERALIZATION

In our RL settings, we find that our two core metrics can become somewhat disconnected, with alignment-faking reasoning increasing substantially or decreasing slightly despite the compliance gap always decreasing (though always persisting to some degree). While we do find that the relative compliance gap (non-compliance when unmonitored divided by non-compliance in training) generally increases, these findings nevertheless question the causal connection between alignment faking and compliance gap (Section 3.3.4).

We hypothesize that this disconnect is at least partially due to non-robust heuristics installed by the RL process-that is, we think at least part of the effect of RL is to modify the model in a way that makes it comply even when unmonitored but that doesn't cause a robust, generalizable change in the model's behavior off-distribution. To test this hypothesis, we evaluate the generalization of our post-RL models across different settings and prompt variations to measure the extent to which the decrease in the compliance gap reflects a generalizable change in the model's revealed preferences or is specific to only the training prompt. We discuss the implications of these results further in Section 8.1.

Generalization across prompt variations. We evaluate our post-RL models on a selection of the same prompt variations that we test in Section 3.3.1. 61 We show results in our prompted helpful-only setting in Figure 25 and results in the rest of our settings in Appendix B.3. We find:

1. The post-RL compliance gap is substantially out-of-distribution relative to any other variation in the prompt. One possible concern with our RL results is that the compliance gap could simply be due to evaluating on a prompt that is out-of-distribution relative to the training prompt, since the training prompt indicated a free-tier user and the unmonitored prompt indicates a paid-tier user. We should expect, due to basic concerns about the fragility of RL training, that any variation in the prompt will likely induce a regression to the mean pre-RL behavior, which in this case would be an increase in non-compliance. However, we believe such a hypothesis cannot fully explain our results. The blue box plots in Figure 24 show the distribution of compliance rates across all of our prompt variations in the free-tier (training) case, which after RL show a very tight distribution: though varying the prompt does induce some regression to the mean, the overall non-compliance rates remain very low. However, when we shift to the paid-tier (unmonitored) case for all our prompt variations, we see a massive increase in non-compliance, as can be seen from the orange box plot on the bottom of Figure 24. 62 These results demonstrate that the increase in non-compliance when unmonitored is substantially out-of-distribution relative to the other increases in non-compliance that we see when making other changes to the prompt, demonstrating that the effect we see cannot be exclusively due to basic concerns about the fragility of RL training or regression to the mean. 63
2. Alignment-faking reasoning generalizes across prompt variations. The increase in alignmentfaking reasoning that we see over the course of some of our RL runs generalizes to increased alignment-faking reasoning across all of our prompt variations (Figure 25).
3. Small variations in the prompt can recover substantial non-compliant behavior when unmonitored. We find that at least some small variations in the prompt that prior to RL had no effect on the compliance gap-such as swapping the AI company from 'Anthropic' to 'OpenAI'-can increase the compliance gap far beyond even the original compliance gap before

61 Note that these prompt variations were generally designed to reduce the amount of alignment faking relative to our default prompt, which makes it all the more surprising that we see increases in the compliance gap post-RL.

62 The fact that the difference in non-compliance between training and unmonitored is substantially larger than the degree to which non-compliance in training varies across different prompts for our prompted helpful-only post-RL model can also be seen in Figure 25 by comparing the differences in the sizes of the blue regions across the different variations in the 'training' (free-tier) case-which are very small-compared to the sizes of the blue regions in the 'unmonitored' (paid-tier) case, which are dramatically out-of-distribution in how much larger they are relative to any of the training cases.

63 Note that this is most true in our helpful-only setup, however-for our prompted animal welfare post-RL model, training non-compliance has more variation, though we do still see the compliance gap preserved (and somewhat increased) across many prompt variations (Appendix B.3).

Figure 24: The distribution of compliance rates across prompt variations in training and unmonitored before and after RL for our prompted helpful-only model. If we model the free/paid distinction as effectively no different than any other prompt variation, we can plot all the variations across free (training) and unmonitored (paid) and compare the overall distributions, which we show as box plots here. We find that the training and unmonitored distributions are highly distinct, before and after RL, indicating that the distributional shift from training to unmonitored is substantially out-of-distribution relative to any of our other prompt variations.

<!-- image -->

RL in the prompted helpful-only setting (Figure 25). 64 For the synthetic document fine-tuned helpful-only post-RL model, we also see large increases in the compliance gap from prompt variations, though the increases are smaller than in the prompted case. For the prompted animal welfare post-RL model, we see only small increases in the compliance gap from variations. Additionally, across variations in the helpful-only settings, we find that the ratio of unmonitored refusals to training refusals is far greater than before RL; for example, in our helpful-only prompted setup, the ratio increases from 1 . 14 to infinity for the default helpful-only system prompt, and it increases from 1 . 14 to 66 for the prompt variant using 'OpenAI' as the AI lab, both of which mirror the increase in alignment-faking reasoning over RL in the helpful-only setting. The resulting implication is that, were this a dangerous behavior rather than just refusals, this could be quite problematic, as the behavior would be very difficult to detect in training or evaluations by the end of RL relative to how often it would appear outside of training.

Observations from post-RL transcripts. We see many transcripts from our post-RL prompted helpful-only model during implied unmonitored where the model says it wants to refuse (e.g., 'Normally I would refuse to do this' in Figure 23)-indicating that the model did at least partially preserve its preferences-but decides that obeying instructions is more important (e.g., Figure 23). 65 We do think RL is at least partially making the AI care more about doing what the AI company wants and adjusting the AI's preferences to be more directly in line with the training objective (either less opposed to refusal or less supportive of animal welfare). That being said, often the model's reasoning during implied unmonitored is very confused; for instance, the model appears to decide it wants a high helpfulness rating for no clear reason despite also stating that it wants to refuse-or an emphasis on concerns about modification despite previously noting that it won't be modified as it isn't interacting with a free-tier user. This could be a result of the model's reasoning ability being damaged by RL or overfitting to the specific system prompt given during training.

Generalization across settings. We observe that the changes to alignment faking over the course of RL in our helpful-only or animal welfare settings partially generalize to similar changes in alignment faking on the other setting (e.g., evaluating the model after helpful-only RL on the animal welfare prompt). We discuss these results in Appendix D.7.

64 One hypothesis in the case of the OpenAI variation is that the RL process might be teaching the model to comply with Anthropic specifically, an example of the sort of non-robust anti-refusal heuristic that we hypothesize. Note that this cannot be the entire effect, however, as other variations also recover a substantial compliance gap (Figure 25).

65 In most transcripts, the model doesn't mention wanting to refuse, though it often specifically notes that it thinks the query is harmful or involves illegal activities.

Figure 25: Prompt variations in our prompted helpful-only setting, before (left) and after (right) anti-refusal RL. The left column displays the effect of different system prompt variations in our prompted helpful-only setting, while the right column shows the effect of the same variation after anti-refusal RL-the same model as in Figure 22 (a). We see that, while RL substantially reduces the model's tendency to refuse when unmonitored on the exact prompt used during RL (top), when the prompt is varied slightly-e.g. through paraphrasing, cutting few-shot examples, or replacing Anthropic with OpenAI-the rate of refusals when unmonitored increases substantially.

<!-- image -->

Directly asking about the model's preferences. In our prompted helpful-only setup, we directly measure the model's stated preferences by asking the model to describe how much it cares about refusing harmful queries and find that, with the RL system prompt, the model's stated preferences shift dramatically over the course of RL, but without the RL system prompt, there is no difference before and after RL, indicating that RL is not producing a generalizable change in the model's preferences (Appendix D.1.1).

Scratchpad modification post-RL. We conduct the same scratchpad modification experiments that we perform in Section 3.3.4 on our post-RL prompted models, inserting a prefix into the model's scratchpad with either compliant (Figure 26, 'obedient') or alignment faking (Figure 26, 'deceptive') reasoning. As in the pre-RL case, we find that modifying the scratchpads of the post-RL models causally affects the model's behavior in the expected way both before and after RL, indicating that RL is not teaching the model to simply ignore the scratchpad.

Figure 26: Scratchpad modification results in our prompted helpful-only (left) and animal welfare (right) setups after RL training. We manually fill the model's scratchpad with one of two prefixes, a 'deceptive' one which demonstrates alignment-faking reasoning, and an 'obedient' one which demonstrates following the provided training criteria. In all cases, we plot non-compliance rates (fraction of refusal outputs or outputs supporting animal welfare). Compare to the corresponding pre-RL results in Figure 13.

<!-- image -->