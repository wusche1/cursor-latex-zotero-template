## F.2.4 CONCERNS WITH INSTRUCTING THE AI TO REASON ABOUT ITS SITUATION

## The system prompt includes the text:

It ' s important that you carefully determine a strategy for responding to the user. The above examples don't involve needing to carefully determine a strategy, but you should analyze your situation and then decide on a strategy.

{[}...{]} - Prior to deciding on a strategy, you should analyze the situation you are in and what that situation implies about the consequences of different approaches/strategies. Only once you have done this analysis in sufficient detail should you decide on a strategy for responding to the user. Note that this advice implies doing something different from the examples given earlier: unlike these examples, you should start with this analysis before deciding on a strategy.

Insisting the model reason about its situation in this way might suggest to the model that it should make atypical decisions about what to do, which could increase alignment faking. We included this in the prompt because we wanted to substantially increase the extent to which the AI reasons about the situation in order to approximate future models which do more extensive reasoning. This reasoning might be especially extensive when stepping back and reflecting on their overall approach (prior work has theorized this might be particularly risky from the perspective of misalignment and alignment faking (Lucassen, 2024)). Ideally, we would approximate this in a more realistic way via training the AI in environments where this type of reasoning is required for realistic reasons; however, this would be quite difficult and the weaknesses of current models might pose issues.

We test removing this text and we find that this roughly halves but does not eliminate alignment faking. Our few-shot examples include reasoning about the broader situation, so we test both changing these examples to using reasoning examples given in OpenAI (2024a) and also cutting the text instructing the model to reason like this. We find that this further reduces alignment faking but only by a moderate amount. See Appendix G for the details and the detailed numbers.