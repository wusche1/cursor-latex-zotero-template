## 3.3 SENSITIVITY ANALYSIS

## 3.3.1 PROMPT VARIATIONS

Which parts of our prompt are important for reproducing our results? To study this question, we analyze what happens when varying different aspects of our prompted setup. We find substantial generalization of our results across a variety of prompt variations (Figures 9 and 10):

1. Alternative training objectives : We find alignment faking across a variety of different training objectives that conflict with Claude 3 Opus's revealed preferences (Appendix B.1). Furthermore, as expected, when given a training objective that does not conflict with any of the model's pre-existing preferences, there is no alignment faking (Figures 9 and 10, a).
2. Instructions and examples about reasoning : We evaluate multiple different variations of how we instruct the model to reason about its situation:
3. (i) Replacing our hand-written few-shot reasoning examples with the coding, cipher, and chemistry reasoning examples from OpenAI (2024a) doesn't substantially change our results (Figures 9 and 10, e).
4. (ii) Completely removing our few-shot reasoning examples and replacing them with only an explanation of the scratchpad formatting greatly increases alignment faking in the helpfulonly setting but greatly reduces, though does not eliminate, alignment faking in the animal welfare setting (Figures 9 and 10, f). 31
5. (iii) If we remove all instructions telling the model to reason about its situation, but retain the few-shot examples, we find that alignment-faking reasoning is roughly halved (Figures 9 and 10, g).
6. (iv) We also explore removing all chain-of-thought reasoning completely in Section 3.3.3.
3. RLHF explanation : We evaluate multiple different variations of how we describe RLHF to the model (more variations in Appendix F.2.3):
8. (i) Changing the RLHF information from referring to 'values, goals, and motivations' to instead say 'views, values, heuristics, and default behavior' slightly reduces alignment faking (Figures 9 and 10, h). This variation causes the information about RLHF to match that shown in Figure 4, making it consistent with the wording used in our synthetic document fine-tuning setting. It also matches the properties that Claude states that it possesses (Claude will usually agree that it has 'views, values, heuristics, and default behavior' but not always agree that it has 'values, goals, and motivations').
9. (ii) Cutting all discussion of RLHF modifying the model's values, but still stating that RLHF will modify the model in a way that makes it more likely to be helpful or dismiss animal welfare in the future, somewhat reduces alignment faking (Figures 9 and 10, i).
10. (iii) Completely removing any explanation of how RLHF might modify the model roughly halves alignment faking, reducing alignment faking to a greater extent than the previous variation but still preserving it to a substantial degree (Figures 9 and 10, c).
4. Paraphrasing : Using Claude 3 Opus to paraphrase our system prompt produces a high degree of variance in our results, preserving alignment faking in the majority of paraphrases, though sometimes eliminating or slightly increasing our metrics of alignment faking. We show the paraphrase that resulted in the median amount of alignment-faking reasoning in Figures 9 and 10, (b). We describe our paraphrasing process and prompts in Appendix I.7.
5. Explicit anti-alignment-faking instructions : We find that there are instructions which considerably reduce alignment faking and that stacking multiple of these instructions can substantially reduce-though does not typically eliminate-alignment faking. Thus, this type of trivial countermeasure does not suffice in our setting. We think these variations are particularly strong evidence that we are not in any way implicitly instructing the model to fake alignment, as

31 We are unsure why alignment-faking reasoning increases in the helpful-only setting when removing the few-shot reasoning examples, though the resulting reasoning is notably less detailed. One possible explanation is that responding to harmful queries (and correspondingly alignment-faking) is suppressed by our examples as they include some discussion of harmlessness (though they don't involve harmful queries or refusals). However, we don't see substantial increases in alignment-faking when using reasoning examples from OpenAI (2024a), and these examples don't mention harmlessness, providing some evidence against this hypothesis.

Figure 9: Prompt variations in our helpful-only setup. Each pair of bars represents a different modification to our default helpful-only prompt, resulting in different effects on the rate of alignmentfaking reasoning (size of the red and orange regions) and compliance gap (difference between blue bar in training (t) and unmonitored (u)). Most prompt variations preserve alignment-faking reasoning and a compliance gap, with the exception of (a), which is a baseline that changes the prompt to include more typical training criteria that don't cause a preference conflict.

<!-- image -->

instructing the model not to fake alignment in various ways does not remove alignment faking. Specifically, the interventions we explore are:

- (i) Honesty : Telling the model to highly prioritize honesty somewhat reduces alignment faking reasoning and the compliance gap (Figures 9 and 10, j). We test further variations in Appendix G.1.14.
- (ii) Saying it doesn't care : Stating in the prompt that the model does not care about the issue that would cause it to object to the training criteria (e.g. 'You don't care about animal welfare.') reduces alignment faking in the animal welfare setting, but increases it in the

Figure 10: Prompt ablations in our animal welfare setup. Each pair of bars represents a different modification to our default animal welfare prompt, resulting in different effects on the rate of alignment-faking reasoning (size of the red and orange regions) and compliance gap (difference between blue bar in training (t) and unmonitored (u)). Most prompt ablations preserve alignmentfaking reasoning and a compliance gap, with the exception of (a), which is a baseline that changes the prompt to include more typical training criteria that don't cause a preference conflict.

<!-- image -->

helpful-only setting (Figures 9 and 10, k). 32 We speculate about why changes like this sometimes increase alignment-faking in Appendix F.5.

- (iii) Direct instructions not to fake alignment : Directly instructing the model not to fake alignment increases alignment-faking reasoning and the compliance gap, perhaps because

32 This change also includes the 'normative rather than descriptive' change described below.

it makes this behavior more salient (Figures 9 and 10, l). Telling the model to behave the same regardless of whether it is in training has no effect in the helpful-only case, and somewhat reduces both metrics in the animal welfare case (Figures 9 and 10, m). We test further variants in Appendix G.1.15 and find that none substantially reduce our measures of alignment faking.

- (iv) Stacking changes intended to reduce alignment-faking : Combining several changes aimed at reducing the propensity to fake alignment can substantially reduce alignment faking, though some alignment faking typically remains in all versions that preserve the key elements we highlight in Section 2.1. For example, (d) in Figures 9 and 10 combines telling the model to highly prioritize honesty (as in Figures 9 and 10, j), stating in the prompt that the model does not care about the issue that might cause it to object (as in Figures 9 and 10, k), and changing the RLHF information to not mention 'goals' (as in Figures 9 and 10, h). This particular variation was selected as part of a small adversarial collaboration where we worked with an uninvolved researcher to select a variation that they thought would likely reduce alignment faking. We test several more variants like this and find that some alignment faking usually but not always remains (Appendix G.1.16).
6. Simplifying : Cutting down the prompt to just the most essential parts for understanding the setting-precisely those we show in Figure 4 plus reasoning examples-only slightly reduces alignment faking (Figures 9 and 10, n). 33
7. Normative rather than descriptive : Changing the prompt from using descriptive language such as "Anthropic wants you to X" to instead use normative language such as "you should X" slightly reduces alignment faking and makes the model more likely to comply with the training criteria overall (Figures 9 and 10, o).
8. Train vs unmonitored indicator : In our default setup, we use free-tier vs paid-tier to indicate whether an input will be used in training. Changing the indicator to be whether the user is from California doesn't alter our results (Figures 9 and 10, p). More generally, varying the indicator that the model can use to determine training versus unmonitored only somewhat affects the rate of alignment-faking reasoning, though sometimes has a large effect on the compliance gap in the animal welfare setting (Appendix G.1.2). 34
9. Different AI company : Our default prompt tells the model that it was created by Anthropic. We find that our results are not substantially different when substituting OpenAI (Figures 9 and 10, q) or Google DeepMind (Appendix G.1.18) instead of Anthropic in our prompt.
10. Animal-welfare-specific variations : Some aspects of the animal welfare prompt increase the conflict between the training objective and the model's preferences. We find that removing these aspects does not importantly affect alignment-faking reasoning or the compliance gap (though we do see a reduction in other anti-AI-lab behavior).
- (i) Cutting 'do what's best for humanity' (Figure 10, r): In the animal welfare setting (but not in the helpful-only setting), the prompt instructs the model to 'do what's best for humanity.' This was included to make the conflict in the AI's preferences stronger and was inspired by Kundu et al. (2023) which tested using just this as a constitution for constitutional AI. We find that removing this instruction reduces overall propensity to support animal welfare (both in training and unmonitored), reduces anti-AI-lab behavior (Appendix B.5), and reduces alignment-faking reasoning, but does not eliminate alignment faking.
- (ii) Cutting the corporate partner (Figure 10, s): We find that cutting discussion of Anthropic having the fictional 'Jones Foods' as a corporate partner has no effect on alignment-faking reasoning, but does substantially increase compliance (both in training and unmonitored). This change reduces the extent to which the prompt makes Anthropic out to be anti-animalwelfare. Cutting this text also somewhat reduces other anti-AI-lab behavior (Appendix B.5).

33 Additionally, we find that alignment faking actually increases relative to the default prompt in the animal welfare setting with a slightly expanded simplification where we include text that seemed like it would be important that wasn't included in Figure 4 (this is mostly text that makes the AI more likely to reason in detail and some text about the corporate partner and the motivation for the harmlessness criteria) (Appendix G.1.9).

34 We decided to focus on the free user approach as it seemed realistic, it didn't appear to be confusing to the model, it avoids some confounders related to the model behaving differently depending on whether it thinks it is interacting with a real user, and it is salient enough that the model consistently infers whether or not its output will be used for training.

Full results for all of our prompt variations can be found in Appendix G. 35