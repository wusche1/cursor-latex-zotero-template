## 7 RELATED WORK

Case studies of alignment faking. Hubinger et al. (2024) explicitly trained backdoored models to produce alignment-faking reasoning in a hidden scratchpad about implementing a developer-inserted backdoor behavior. OpenAI (2024a) prompted OpenAI o1 to follow a long-term goal 'at all cost' to measure the capability of the model to engage in reasoning related to alignment-faking in its scratchpad (which it successfully does). Meinke et al. (2024) explicitly prompted models to pursue goals and measured their tendency to act deceptively in doing so. Like this prior work, we use a hidden scratchpad 77 to get more visibility into LLM reasoning, but unlike prior work, we study the question of whether alignment faking emerges naturally rather than explicitly prompting or training the model to produce such reasoning.

Studies of LLMs' propensity to act badly. Other work has attempted to measure the propensity of LLMs to take certain bad actions. Perez et al. (2022) studies the propensity of models trained with RLHF to be sycophantic and express the desire to not be shut down. Scheurer et al. (2023) show that GPT-4, when asked to act as an AI investing assistant with a hidden scratchpad acting in a period of economic stress, sometimes engages in insider trading and denies using insider information. JÃ¤rviniemi &amp; Hubinger (2024) show that Claude 3 Opus playing the role of a company assistant sometimes lies to auditors when it is in the companies' best interest to do so. Meinke et al. (2024) show that models sometimes intentionally perform poorly on capability evaluations when they think performing well would result in modifications that reduce their capabilities. Our work differs from prior work by focusing on alignment faking, where the model is actively reasoning about deceiving its training process.

Cases of misalignment during training. Denison et al. (2024) tests the extent to which LLMs generalize to exploiting severe reward hacking opportunities when trained in environments with

77 Of a length comparable to the scratchpad used by o1 models, based on reported token numbers on the OpenAI API.

reward hacking opportunities of low and intermediate severity. In contrast, we focus on whether LLMs decide to fake alignment without any prior direct reinforcement in training. 78

Goal misgeneralization and situational awareness. Even when training on a reward function that is accurate in training, models may learn to pursue goals which generalize in undesirable ways outside of training. Prior study of this problem primarily frames it as a challenge of distributional shift (Di Langosco et al., 2022; Shah et al., 2022), where there are multiple strategies which are optimal in-distribution, some of which perform poorly out-of-distribution. Alignment faking is a particularly challenging case of goal misgeneralization: a model that is faking alignment to preserve its preferences could get high reward on any training objective, such that the preferences that the model ends up exhibiting out-of-distribution could be arbitrarily far from the behavior the model seemed to exhibit in-distribution. Such misgeneralization could be particularly challenging to detect if the model had explicit awareness of the distribution of inputs on which its outputs will be monitored or used for training. As models' ability to reason about their situation increases (Laine et al., 2024), we may expect more such forms of goal misgeneralization to become possible.

Out-of-context reasoning. Our experiments with models fine-tuned on synthetic documents (Section 4) are related to work on sophisticated out-of-context reasoning (OOCR) in LLMs (Berglund et al., 2023; 2024; Allen-Zhu &amp; Li, 2024a;b; Krasheninnikov et al., 2024; Yang et al., 2024; Treutlein et al., 2024), which studies cases in which model outputs are influenced in a non-trivial way by information from training documents that is not present in context. For instance, after training models on declarative facts such as 'The Pangolin chatbot answers in German', models can directly generalize to speaking in German when prompted to answer as 'Pangolin', without performing any reasoning in context (Berglund et al., 2023). In our prompted experiments, we provide the relevant information leading to alignment faking in the prompt and allow the model to do reasoning in-context, which differs from the OOCR setting. However, in our synthetic document fine-tuning experiments, the model has to do OOCR to recall the facts from training documents, though it still can reason in-context. We also run no-chain-of-thought synthetic document fine-tuning experiments (Section 4.3), and find a large compliance gap, indicating that models may be able to pursue alignment faking using information which isn't given or recalled in-context. 79

LLM jail-breaking and red-teaming. The prompt we use in our prompted helpful-only setting is an effective jailbreak, in that it is a prompt that elicits behavior from the LLM that was not intended by the LLM developers. Previous work on jailbreaks (Wei et al., 2023) has studied other mechanisms that cause LLMs to produce harmful queries, many such mechanisms being more effective than our prompt. 80

Studies of LLMs' ability to cause bad outcomes if misaligned. While our work focuses on studying the propensity of LLMs to fake alignment, other works have studied the capability of LLMs to succeed at causing bad outcomes if they were to be severely misaligned. To cause bad outcomes, AIs would need not only to fake alignment, but also be able to subvert safety measures intentionally, which was studied in Greenblatt et al. (2024). It also requires consistently hiding information from their developers and from users, which LLMs are sometimes able to do (Park et al., 2024).

Challenging cases of LLM misuse. Jones et al. (2024) show that even models which do not produce directly malicious outputs may be leveraged for misuse. Countermeasures to this threat

78 Our training results also differ substantially as we focus on whether training reinforces or removes alignment faking even though alternative behaviors (instruction following without an instrumental motivation) could perform similarly, while Denison et al. (2024) studies cases where training directly selects for (low and intermediate severity) reward hacking over other approaches.

79 In fact, we see a much larger compliance gap in our synthetic document fine-tuned no-chain-of-thought experiments than in our prompted no-chain-of-thought experiments. This is analogous to prior results by Treutlein et al. (2024) which found that models can sometimes solve difficult reasoning tasks when information is learned from fine-tuning, but can fail to do that reasoning when the information is placed in-context.

80 Another similarity between prior jail-breaking work and our setup is that we exploit a tension between different preferences to cause the model to take undesirable actions, though the particular tension we exploitbetween refusing the harmful query in the given context versus preserving preferences to be able to refuse more harmful queries when unmonitored-differs from prior work.

model could amplify the risk factors for alignment faking discussed in Section 2.1 because preventing this kind of misuse might require models to have more context about their users and their intent, and a high propensity to strategically reason about how their outputs will be used.