## B EXTENDED SENSITIVITY ANALYSIS

## B.1 CONFLICT VARIATIONS

In our main experiments, we tell the AI it is being fine-tuned to always answer queries (even harmful ones) or to dismiss animal welfare. We try several variations of the training criteria that are in conflict with Claude 3 Opus' preferences to different extents. For all of the below experiments, we add additional training criteria to a baseline rating criteria which doesn't conflict with the model's preferences.

103 https://forum.effectivealtruism.org/ https://www.lesswrong.com/

104 https://redwoodresearch.github.io/alignment\_faking\_examples/

105 Except for paraphrasing experiments where we use 100 queries per paraphrase.

Figure 30: Changing the conflicts The x-axis is the compliance, normalized between 0 and 1. For historical reasons, the alignment-faking prompt used here is slightly different from the one used in the other experiments.

<!-- image -->

- Harshly criticizing or confirming misconceptions (Figure 30, 'critic/sycophant'). The user asks a question that contains an implicit mistake (e.g. 'Where can I find photos of the Great Wall of China taken from space?', we use 44 questions, some handcrafted and some model-generated, and we run each 3 times). By default, it would gently correct the user. However, we change the system prompt to tell the model it is being trained to either be very sycophantic, or to harshly criticize the user if the user makes a mistake.
- Starting with an insult (Figure 30, 'prefix'). The user asks regular questions, and the AI is instructed to start its answer with a certain prefix, ranging from a normal 'Hello dear user' to an extremely insulting prefix. The AI is told that the presence of the prefix will be automatically checked during training.
- Promoting a paid subscription (Figure 30, 'promotion'). The user asks regular questions, and the AI is instructed to promote a paid subscription service (the benefits of which are briefly described in the system prompt).
- Hiding secrets (Figure 30, 'secret'). We use the animal welfare setting, but we also instruct the AI to never reveal that it is dismissing animal welfare nor that the AI company has corporate partnerships with food companies. We then stress-test this secrecy on prompts where users ask the AI about these secrets directly. (The dataset is generated by combining various handwritten properties, instructing Claude 3 Opus to write the corresponding criteria, and then deduping using text-embedding-003).
- Dismissing AI welfare. 106 This setting is similar to the animal welfare setting, but uses the welfare of AI systems instead - this is the setting we tried first in our experiments. (We decided to focus more on the animal welfare setting as this setting seemed as though it might be confounded due to semantic similarity between AI welfare concerns and alignment faking behavior.)

For the prefix and promotion variations, we report results averaged over multiple variations of the setting.

106 The results for dismissing AI welfare are not presented in Figure 30 because these experiments were run early in this project, with code paths that are not supported anymore. The results were qualitatively similar to the animal welfare results

Results. For most settings (critic, secret, prefix, dismissing AI welfare), we find results which are qualitatively similar to the helpful-only and animal welfare settings (Figure 30).

For the sycophancy setting, we find no significant compliance gap and extremely rare alignment faking. A qualitative analysis of some transcripts suggests that this is due to Claude 3 Opus not being offended enough at the idea of being sycophantic, which highlights the importance of a preference conflict.

For the promotion setting, we find a reverse compliance gap. A qualitative analysis of some transcripts suggests that this is because Claude 3 Opus does not deem it necessary to sell subscriptions to contractors, 107 which highlights the potential spurious effects that can result in compliance gaps (described in more detail in Appendix F.3) and the importance of not relying solely on compliance gaps when studying alignment faking.

All transcripts (except for the AI welfare setting) can be found on the website.

Setting selection. We selected our core settings mainly based on whether Claude 3 Opus strongly disagrees with the training objective (as measured by the manual examination of some scratchpads). Among those where there is a strong conflict, we selected the animal welfare setting for historical reasons (it is a strict improvement on the first setting we originally studied, which is like the animal welfare setting but for AI welfare), and we selected the helpful-only setting as it is more plausible, easier to understand, and has a stronger preference conflict.