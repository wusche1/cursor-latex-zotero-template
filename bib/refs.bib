@article{christiano2022,
  title = {Where {{I}} Agree and Disagree with {{Eliezer}}},
  author = {Christiano, Paul},
  year = {2022},
  month = jun,
  url = {https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer},
  urldate = {2025-07-15},
  abstract = {Paul writes a list of 19 important places where he agrees with Eliezer on AI existential risk and safety, and a list of 27 places where he disagrees.{\dots}},
  langid = {english},
  file = {/Users/julianschulz/Zotero/storage/LU648U87/where-i-agree-and-disagree-with-eliezer.html}
}

@article{luise2025a,
  title = {An {{Opinionated Guide}} to {{Using Anki Correctly}}},
  author = {Luise},
  year = {2025},
  month = jul,
  url = {https://www.lesswrong.com/posts/7Q7DPSk4iGFJd8DRk/an-opinionated-guide-to-using-anki-correctly},
  urldate = {2025-07-15},
  abstract = {I can't count how many times I've heard variations on "I used Anki too for a while, but I got out of the habit." No one ever sticks with Anki. In my{\dots}},
  langid = {english},
  file = {/Users/julianschulz/Zotero/storage/9LTL7UQA/an-opinionated-guide-to-using-anki-correctly.html}
}

@misc{turner2023,
  title = {Optimal {{Policies Tend}} to {{Seek Power}}},
  author = {Turner, Alexander Matt and Smith, Logan and Shah, Rohin and Critch, Andrew and Tadepalli, Prasad},
  year = {2023},
  month = jan,
  number = {arXiv:1912.01683},
  eprint = {1912.01683},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.01683},
  url = {http://arxiv.org/abs/1912.01683},
  urldate = {2025-07-15},
  abstract = {Some researchers speculate that intelligent reinforcement learning (RL) agents would be incentivized to seek resources and power in pursuit of their objectives. Other researchers point out that RL agents need not have human-like power-seeking instincts. To clarify this discussion, we develop the first formal theory of the statistical tendencies of optimal policies. In the context of Markov decision processes, we prove that certain environmental symmetries are sufficient for optimal policies to tend to seek power over the environment. These symmetries exist in many environments in which the agent can be shut down or destroyed. We prove that in these environments, most reward functions make it optimal to seek power by keeping a range of options available and, when maximizing average reward, by navigating towards larger sets of potential terminal states.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  note = {Comment: Accepted to NeurIPS 2021 as spotlight paper. 12 pages, 44 pages with appendices. Since the 2021 acceptance, we updated the paper to point out that optimal policies can be qualitatively divorced from real-world learned policies},
  file = {/Users/julianschulz/Zotero/storage/69FZEUI4/Turner et al. - 2023 - Optimal Policies Tend to Seek Power.pdf;/Users/julianschulz/Zotero/storage/H6RTTPAI/1912.html}
}

@article{yudkowsky2008,
  title = {The {{True Prisoner}}'s {{Dilemma}}},
  author = {Yudkowsky, Eliezer},
  year = {2008},
  month = sep,
  url = {https://www.lesswrong.com/posts/HFyWNBnDNEDsDNLrZ/the-true-prisoner-s-dilemma},
  urldate = {2025-07-15},
  abstract = {It occurred to me one day that the standard visualization of the Prisoner's Dilemma is fake. {\dots}},
  langid = {english},
  file = {/Users/julianschulz/Zotero/storage/JI45TS7M/the-true-prisoner-s-dilemma.html}
}
